{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate our own dataset for the Franke Function, with $x,y\\in[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our own code to perform a standard least square regression analysis usning polynomials in x and y up to fith order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import norm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#from sklearn.utils.testing import ignore_warnings\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from imageio import imread\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "np.random.seed(np.random.randint(1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankeFunction(x, y, sigma = 0):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "\n",
    "    noise = np.random.normal(0, 1, x.shape[0])\n",
    "    noise = noise.reshape(x.shape[0],1)\n",
    "\n",
    "    return (term1 + term2 + term3 + term4).reshape(-1,1)  + sigma*noise\n",
    "\n",
    "def create_X(x, y, n ):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2)   # Number of elements in beta\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X\n",
    "\n",
    "def least_square(x_value,y_value, *args, **kwargs):\n",
    "    # Using pinv\n",
    "    return np.linalg.pinv(x_value.transpose().dot(x_value)).dot(x_value.transpose().dot(y_value))\n",
    "\n",
    "def plot_errors(\n",
    "    x_range_train, \n",
    "    x_range_test, \n",
    "    y_values_train, \n",
    "    y_values_test, \n",
    "    title, \n",
    "    xlabel_axis, \n",
    "    ylabel_axis, \n",
    "    graph_label_train, \n",
    "    graph_label_test, \n",
    "    y_scale = 'linear',\n",
    "    task_file_name=None,\n",
    "):\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize = (16,9))\n",
    "    plt.xlabel(xlabel_axis)\n",
    "    plt.ylabel(ylabel_axis)\n",
    "    plt.title(title)\n",
    "    plt.yscale(y_scale)\n",
    "    plt.plot(x_range_train, y_values_train, label=graph_label_train)\n",
    "    plt.plot(x_range_test, y_values_test, label=graph_label_test)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if task_file_name is not None:\n",
    "        plt.savefig(f\"figures/{task_file_name}.png\", dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mse_and_r2_by_complexity(\n",
    "    reg_func = least_square,\n",
    "    x = None, \n",
    "    y = None, \n",
    "    z = None,\n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0.1, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False,\n",
    "    y_scale = 'linear',\n",
    "    lamb = 0,\n",
    "    return_losses = False,\n",
    "    scale_with_std = False,\n",
    "    task_file_name = None):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if x is None:\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "    if z is None:\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=scale_with_std)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=scale_with_std)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        all_ols_betas.append(beta_opt)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(beta_opt)\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "    \n",
    "    if plot_mse:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test',\n",
    "            y_scale = y_scale,\n",
    "            task_file_name = f\"{task_file_name}_MSE\",\n",
    "        )\n",
    "\n",
    "    if plot_r2:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test',\n",
    "            task_file_name = f\"{task_file_name}_r2_score\",\n",
    "        )\n",
    "    \n",
    "    if return_losses:\n",
    "        return MSE_pred, r2_pred\n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n",
    "\n",
    "betas, xtx = simple_mse_and_r2_by_complexity(num_points = 500, complexity = 20, noise = 0.1, plot_mse = True, plot_r2 = True, task_file_name=\"ex_1_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(beta, xtx_inv, z_score):\n",
    "    \"\"\"\n",
    "    Calculates the confidence interval of our parameters beta\n",
    "    \n",
    "    Args:\n",
    "        beta: our parameters\n",
    "        xtx_inv: (pseudo) inverted of our design matrix transposed multiplied with the design matrix.\n",
    "        confidence: confidence level\n",
    "    Returns:\n",
    "        confidence_interavl: the confidence interval for each of our parameters for a given z_score\n",
    "    \"\"\"\n",
    "    diag_sqrt = np.sqrt(np.diag(xtx_inv)).reshape(-1,1)\n",
    "\n",
    "    \n",
    "    confidence_interval = [beta-z_score*diag_sqrt, beta+z_score*diag_sqrt]\n",
    "        \n",
    "    return np.array(confidence_interval)\n",
    "\n",
    "\n",
    "def all_confidence_intervals(num_points = 1000, complexity = 5, noise = 10):\n",
    "    betas, xtx = simple_mse_and_r2_by_complexity(num_points = num_points, complexity = complexity, noise = noise)\n",
    "\n",
    "    for i, beta in enumerate(betas):\n",
    "        interval_min, interval_max = confidence_interval(beta, xtx[i], 1.96)\n",
    "        print(\"Complexity:\",i+1)\n",
    "        data = {\n",
    "            'min': interval_min.ravel(),\n",
    "            'betas': beta.ravel(),\n",
    "            'max': interval_max.ravel()\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df)\n",
    "        df.to_csv(f'./data/conf_int_complexity{i+1}.csv')\n",
    "\n",
    "all_confidence_intervals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Figure 2.11 Using log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, xtx = simple_mse_and_r2_by_complexity(num_points = 500, complexity = 15, noise = 1, plot_mse=True, y_scale = 'linear', task_file_name=\"ex_2_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance trade-off with bootstrap and varying number of datapoints ( for complexity, test size, and number of bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse_with_broadcast(y_pred, y_test):\n",
    "    return np.mean((y_test - y_pred)**2, axis=1, keepdims=True)\n",
    "\n",
    "def bias_variance_analysis_bootstrap(reg_func, \n",
    "                                     num_points, \n",
    "                                     max_degree, \n",
    "                                     num_bootstraps,\n",
    "                                     x = None,\n",
    "                                     y = None,\n",
    "                                     z = None,\n",
    "                                     noise = 0.1,\n",
    "                                     lamb = 0, \n",
    "                                     plot = True, \n",
    "                                     plot_log = True, \n",
    "                                     show_plot = True, \n",
    "                                     test_size = 0.2, \n",
    "                                     only_return_error = False, \n",
    "                                     extra_text = \"\", \n",
    "                                     save_fig = False,\n",
    "                                     scale_with_std = False):\n",
    "        \n",
    "    # Make data\n",
    "    if x is None:\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "    if z is None:\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "    \n",
    "    # Aggregrate results\n",
    "    error = np.zeros(max_degree)\n",
    "    bias = np.zeros(max_degree)\n",
    "    variance = np.zeros(max_degree)\n",
    "    \n",
    "    # to find best model\n",
    "    best_mse = float('inf')\n",
    "    best_complexity = 0\n",
    "    \n",
    "    for complexity in range(max_degree):\n",
    "        \n",
    "        # Make design matrix\n",
    "        X = create_X(x, y, n=complexity + 1) # +1, as we want to start from complexity 1\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=test_size)\n",
    "        \n",
    "        # Scaling the data\n",
    "        scaler_in = StandardScaler(with_std=scale_with_std)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=scale_with_std)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        X_train = scaler_in.transform(X_train)\n",
    "        X_test = scaler_in.transform(X_test)\n",
    "        z_train = scale_z.transform(z_train)\n",
    "        z_test = scale_z.transform(z_test)\n",
    "        \n",
    "        # Bootstrap\n",
    "        agg = []\n",
    "        z_pred_aggregate = np.empty((z_test.shape[0], num_bootstraps))\n",
    "        for boot_strap_number in range(0, num_bootstraps):\n",
    "            x_sample, y_sample = resample(X_train, z_train)\n",
    "            # Find optimal betas\n",
    "            beta_opt = reg_func(x_sample, y_sample, lamb)\n",
    "            # Predict\n",
    "            z_pred = X_test.dot(beta_opt).ravel()\n",
    "            # Aggregate the predictions\n",
    "            z_pred_aggregate[:, boot_strap_number] = z_pred\n",
    "            \n",
    "        # Loss: Mean of all the bootstrap MSE's\n",
    "        #z_pred_aggregate = np.delete(z_pred_aggregate, 0, 1)\n",
    "        mse_test = np.mean(custom_mse_with_broadcast(z_pred_aggregate, z_test))\n",
    "        # Find best model\n",
    "        if mse_test < best_mse:\n",
    "            best_mse = mse_test\n",
    "            best_complexity = complexity + 1 #Start from complexity 1, but index from 0\n",
    "        #print(z_pred_aggregate[:,complexity])\n",
    "        # Aggregrate stats:\n",
    "        error[complexity] = mse_test\n",
    "        bias[complexity] = np.mean((z_test - np.mean(z_pred_aggregate, axis = 1, keepdims=True))**2)\n",
    "        variance[complexity] = np.mean((z_pred_aggregate - np.mean(z_pred_aggregate, axis = 1, keepdims = True))**2)\n",
    "    \n",
    "    # Calculate bias + variance\n",
    "    bias_annd_variance = np.add(bias.copy(), variance.copy())\n",
    "    \n",
    "    # Plot figure of bias variance\n",
    "    polydegree =  np.arange(1, max_degree + 1)\n",
    "    if plot:\n",
    "        plt.figure(figsize = (15,5))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(polydegree, error, label='Error')\n",
    "        plt.plot(polydegree, bias, label='bias')\n",
    "        plt.plot(polydegree, variance, label='Variance')\n",
    "        plt.plot(polydegree, bias_annd_variance, label = 'bias + variance')\n",
    "        plt.title(f\"{reg_func.__name__}: Bias-Variance in linear scale, Num_datapoints = {num_points}\"+ extra_text)\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.yscale('linear')\n",
    "        plt.legend()\n",
    "        if save_fig:\n",
    "            plt.savefig(f\"./figures/{reg_func.__name__}_bias_variance_Num_datapoints_{num_points}.png\", dpi=150)\n",
    "        plt.show()\n",
    "    if plot_log:\n",
    "        plt.figure(figsize = (20,5))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(polydegree, error, label='Error')\n",
    "        plt.plot(polydegree, bias, label='bias')\n",
    "        plt.plot(polydegree, variance, label='Variance')\n",
    "        plt.plot(polydegree, bias_annd_variance, label = 'bias + variance')\n",
    "        plt.title(f\"{reg_func.__name__}: Bias-Variance in Log scale, Num_datapoints = {num_points} \" + extra_text)\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    #print(f\"Best MSE: {best_mse} at complexity: {best_complexity} for number of datapoints: {num_points}, num_bootstraps = {best_num_bstraps}, , test_split = {train_split}\")\n",
    "    if only_return_error:\n",
    "        return error\n",
    "    \n",
    "    return best_mse, best_complexity, error, bias, variance\n",
    "\n",
    "def run_bias_variance_bootstrap(reg_func, \n",
    "                                 x = None,\n",
    "                                 y = None,\n",
    "                                 z = None,\n",
    "                                 noise = 0.1,\n",
    "                                 points = [20, 50, 100], \n",
    "                                 max_degree = 12, \n",
    "                                 test_sizes = (0.2, 1, 1), \n",
    "                                 lamb = 0, \n",
    "                                 plot_best = True,\n",
    "                                 extra_text = \"\",\n",
    "                                 save_fig = False,\n",
    "                                 plot_scale = 'linear'):\n",
    "    \n",
    "    best_mse_per_points = np.zeros(len(points))\n",
    "    best_complexity_per_points = np.zeros(len(points))\n",
    "    \n",
    "    best_error_per_points = np.zeros((len(points), max_degree))\n",
    "    best_bias_per_points = np.zeros((len(points), max_degree))\n",
    "    best_variance_per_points = np.zeros((len(points), max_degree))\n",
    "    best_train_split_per_points = np.zeros(len(points))\n",
    "    \n",
    "    # Run for different number of datapoints\n",
    "    for point_ind, num_points in enumerate(points):\n",
    "        global_best_mse = float('inf')\n",
    "        global_best_complexity = 0\n",
    "        best_num_bstraps = 0\n",
    "        best_train_split = 0\n",
    "        best_error = []\n",
    "        best_bias = []\n",
    "        best_variance = []\n",
    "        \n",
    "        #for num_bstraps in range(5,11):\n",
    "        # run for different size of test data\n",
    "        for test_size in np.arange(start = test_sizes[0], stop = test_sizes[1], step = test_sizes[2]):\n",
    "            best_mse, best_complexity, error, bias, variance = bias_variance_analysis_bootstrap(reg_func, num_points, x=x, y=y, z=z, noise = noise, max_degree=max_degree, num_bootstraps = num_points, test_size = test_size, plot=False, plot_log = False, lamb = lamb, extra_text = extra_text)\n",
    "            if best_mse < global_best_mse:\n",
    "                global_best_mse = best_mse\n",
    "                global_best_complexity = best_complexity\n",
    "                best_train_split = test_size\n",
    "                best_error = error\n",
    "                best_bias = bias\n",
    "                best_variance = variance\n",
    "                \n",
    "        # Re-run and plot best results for each number of datapoints\n",
    "        if plot_best:\n",
    "            bias_annd_variance = np.add(best_bias.copy(), best_variance.copy())\n",
    "            polydegree =  np.arange(1, max_degree + 1)\n",
    "            print(f\"Best global: {global_best_mse} at complexity: {global_best_complexity} for number of datapoints: {num_points}, num_bootstraps = {num_points}, test_split = {best_train_split}\")\n",
    "            plt.figure(figsize = (15,5))\n",
    "            plt.plot(polydegree, best_error, label='Error')\n",
    "            plt.plot(polydegree, best_bias, label='bias')\n",
    "            plt.plot(polydegree, best_variance, label='Variance')\n",
    "            plt.plot(polydegree, bias_annd_variance, label = 'bias + variance')\n",
    "            plt.title(f\"{reg_func.__name__}: Bias-Variance in linear scale, Num_datapoints = {num_points}\"+ extra_text)\n",
    "            plt.xlabel(\"Complexity\")\n",
    "            plt.ylabel(\"Error\")\n",
    "            plt.yscale(plot_scale)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        best_mse_per_points[point_ind] = global_best_mse\n",
    "        best_complexity_per_points[point_ind] = global_best_complexity\n",
    "        \n",
    "        best_error_per_points[point_ind,:] = best_error\n",
    "        best_bias_per_points[point_ind,:] = best_bias\n",
    "        best_variance_per_points[point_ind,:] = best_variance\n",
    "        best_train_split_per_points[point_ind] = best_train_split\n",
    "\n",
    "    return best_mse_per_points, best_complexity_per_points, best_error_per_points, best_bias_per_points, best_variance_per_points, best_train_split_per_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_bias_variance_bootstrap(least_square, test_sizes = (0.2, 0.5, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data after we split the data, as if we split it before, then the we scale based on all the data instead of just training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_custom(reg_func, \n",
    "                 num_points, \n",
    "                 num_splits, \n",
    "                 complexity, \n",
    "                 x=None, \n",
    "                 y=None,\n",
    "                 z=None,\n",
    "                 noise = 0.1, \n",
    "                 lamb = 0, \n",
    "                 scale = True,\n",
    "                 scale_with_std = False,\n",
    "                 set_random_seed=False):\n",
    "    \n",
    "    # Set random seed to compare with SKlearn\n",
    "    if set_random_seed:\n",
    "        np.random.seed(88)\n",
    "\n",
    "    \n",
    "    # Create data\n",
    "    if x is None:\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "    if z is None:\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "    \n",
    "    X = create_X(x, y, n=complexity)\n",
    "\n",
    "    # Initialize our kfold\n",
    "    kfold = KFold(n_splits = num_splits)\n",
    "    \n",
    "    # Error\n",
    "    mse_test_kfold = np.zeros(num_splits)\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        # Training and test split + scaling\n",
    "\n",
    "        if scale:\n",
    "            if scale_with_std:\n",
    "                X_train = (X[train_index] - np.mean(X[train_index]))/np.std(X[train_index])\n",
    "                z_train = (z[train_index] - np.mean(z[train_index]))/np.std(z[train_index])\n",
    "\n",
    "                X_test = (X[test_index] - np.mean(X[test_index]))/np.std(X[test_index])\n",
    "                z_test = (z[test_index] - np.mean(z[test_index]))/np.std(z[test_index])\n",
    "            else:\n",
    "                X_train = X[train_index] - np.mean(X[train_index])\n",
    "                z_train = z[train_index] - np.mean(z[train_index])\n",
    "\n",
    "                X_test = X[test_index] - np.mean(X[test_index])\n",
    "                z_test = z[test_index] - np.mean(z[test_index])\n",
    "        # Scale = False for comparing with SKlearn\n",
    "        else:\n",
    "            X_train = X[train_index] \n",
    "            z_train = z[train_index]\n",
    "\n",
    "            X_test = X[test_index]\n",
    "            z_test = z[test_index]\n",
    "        \n",
    "        \n",
    "        # Optimal Betas\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "\n",
    "        # Prediction\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "        \n",
    "        mse_test_kfold[i] = mean_squared_error(z_test, z_pred)     \n",
    "        \n",
    "    return np.mean(mse_test_kfold)\n",
    "\n",
    "    \n",
    "def cross_validation(reg_func, \n",
    "                     num_points, \n",
    "                     num_folds, \n",
    "                     max_complexity, \n",
    "                     x=None, \n",
    "                     y=None,\n",
    "                     z=None,\n",
    "                     noise=0.1, \n",
    "                     lamb=0, \n",
    "                     plot = False,\n",
    "                     scale = True, \n",
    "                     set_random_seed=False,\n",
    "                     scale_with_std = False):\n",
    "    \n",
    "    mse_per_complexity = np.zeros(max_complexity)\n",
    "    \n",
    "    for complexity in range(max_complexity):\n",
    "        mse_per_complexity[complexity] = kfold_custom(reg_func, num_points, num_folds, complexity+1, x=x, y=y, z=z, noise=noise, lamb=lamb, scale = scale, set_random_seed = set_random_seed, scale_with_std = scale_with_std)\n",
    "    \n",
    "    if plot:\n",
    "        polydegree =  np.arange(1, max_complexity +1 )\n",
    "        plt.figure(figsize = (16,9))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(polydegree, mse_per_complexity, label='MSE cross validation: Num Folds = ' + str(num_folds))\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.title(\"MSE all folds per complexity\")\n",
    "        #plt.yscale('log')\n",
    "        plt.legend()\n",
    "    \n",
    "    return mse_per_complexity\n",
    "        \n",
    "def full_cross_valid(reg_func, \n",
    "                     num_points, \n",
    "                     min_fold, \n",
    "                     max_fold, \n",
    "                     max_complexity, \n",
    "                     x=None, \n",
    "                     y=None,\n",
    "                     z=None,\n",
    "                     noise=0.1, \n",
    "                     lamb=0, \n",
    "                     plot = False, \n",
    "                     show_plot = False,\n",
    "                     scale = True, \n",
    "                     set_random_seed=False,\n",
    "                     scale_with_std = False):\n",
    "    \n",
    "    all_mse_per_fold = np.zeros([max_fold-min_fold+1, max_complexity])\n",
    "    \n",
    "    # perform CV for different number of folds\n",
    "    for i, fold in enumerate(range(min_fold, max_fold+1)):\n",
    "        all_mse_per_fold[i] = cross_validation(reg_func, num_points, fold, max_complexity, x=x, y=y, z=z, noise=noise, lamb=lamb, plot=plot, scale = scale, set_random_seed = set_random_seed, scale_with_std = scale_with_std)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return all_mse_per_fold\n",
    "\n",
    "def find_best_params_cv(reg_func, \n",
    "                        num_points, \n",
    "                        min_fold, \n",
    "                        max_fold, \n",
    "                        max_complexity,\n",
    "                        x=None,\n",
    "                        y=None,\n",
    "                        z=None,\n",
    "                        noise=0.1, \n",
    "                        lamb=0, \n",
    "                        plot = False,\n",
    "                        show_plot = False, \n",
    "                        set_random_seed = False):\n",
    "    \n",
    "    results = full_cross_valid(reg_func, num_points, min_fold, max_fold , max_complexity, x=x, y=y, z=z, noise=noise, lamb=lamb, plot = True, show_plot = True, set_random_seed = set_random_seed)\n",
    "    best_fold, best_complexity = np.unravel_index(np.argmin(results, axis=None), results.shape)\n",
    "    # Since fold indexing starts from 0, we need to add the min fold to get the actual fold\n",
    "    actual_best_fold = best_fold + min_fold \n",
    "    print(f\"Best params: Folds = {actual_best_fold} and complexity = {best_complexity}\")\n",
    "    polydegree =  np.arange(1, max_complexity+1)\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.tight_layout()\n",
    "    plt.plot(polydegree, results[best_fold], label = f\"MSE for best fold {actual_best_fold}\")\n",
    "    plt.title(f\"MSE for best parameters of CV using {reg_func.__name__}. Best Params: Complexity = {best_complexity}, Best fold = {actual_best_fold}\")\n",
    "    plt.xlabel(\"Complexity\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_params_cv(least_square, 100, 5,10 , 10, plot = True, show_plot = True, set_random_seed = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_sklearn(num_points, num_folds, max_complexity, x=None, y=None, z = None, noise = 0.1, plot=False):\n",
    "    np.random.seed(88)\n",
    "    estimated_mse_sklearn = np.zeros(max_complexity)\n",
    "    \n",
    "    if x is None:\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "    if z is None:\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "    \n",
    "    for complexity in range(max_complexity):\n",
    "        \n",
    "        # Don't scale, just fit the raw data.\n",
    "        linreg = LinearRegression(fit_intercept = False)\n",
    "        \n",
    "        X = create_X(x, y, n=complexity)\n",
    "        \n",
    "        kfold = KFold(n_splits = num_folds)\n",
    "\n",
    "        estimated_mse_folds = cross_val_score(linreg, X, z, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "        estimated_mse_sklearn[complexity] = np.mean(-estimated_mse_folds)\n",
    "        \n",
    "    polydegree =  np.arange(max_complexity)\n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.tight_layout()\n",
    "    plt.plot(polydegree, estimated_mse_sklearn, label='MSE cross validation Sklearn ' + str(num_folds))\n",
    "    plt.xlabel(\"Complexity\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    #plt.yscale('log')\n",
    "    plt.legend()\n",
    "    if plot:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_sklearn(100, 10, 10, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cross_valid_custom_vs_sklearn():\n",
    "    for fold in range(5,11):\n",
    "        full_cross_valid(least_square, 100, fold,fold,15, plot=False, scale = False, set_random_seed = True)\n",
    "        cross_val_sklearn(100, fold, 15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cross_valid_custom_vs_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Cross validation vs. bootstrap for MSE for Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cv_with_booststrap(max_data_points, max_complexity):\n",
    "    for fold in range(5,11):\n",
    "        print(f\"Fold: {fold}\")\n",
    "        #for num_data_points in range(20,max_data_points, 50):\n",
    "        for num_data_points in [20, 100]:\n",
    "            bootstrap_mse = bias_variance_analysis_bootstrap(least_square, num_data_points, max_complexity, num_data_points, plot = False, only_return_error = True, plot_log = False) \n",
    "            cv_mse = cross_validation(least_square, num_data_points, fold, max_complexity)\n",
    "            cross_val_sklearn(num_data_points, fold, max_complexity)\n",
    "            polydegree =  np.arange(max_complexity)\n",
    "            plt.figure(figsize = (16,9))\n",
    "            plt.tight_layout()\n",
    "            plt.plot(polydegree, bootstrap_mse, label='MSE Boot_strap')\n",
    "            plt.plot(polydegree, cv_mse, label='MSE CV num_folds: '+str(fold))\n",
    "            plt.xlabel(\"Complexity\")\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.title(\"Bootstrap vs. CV:  num_data_points: \"+str(num_data_points))\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cv_with_booststrap(200, 10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commment comment comment...\n",
    "\n",
    "Num datapoints: CV better for many, bootstrap better for few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will:\n",
    "* Write our own Ridge regression using singular value decomposition using the function pinv(), which uses the SVD.\n",
    "* Perform the same bootstrap analysis as we did in exercise 2, using the same polynomials with different lambdas.\n",
    "* Perform the same cross validation as in exercise 3, but with varying lambdas.\n",
    "* Compare the results from Ridge (Ex. 4) with results from Least Square (Ex. 1-3).\n",
    "* In general: Study the dependancies of lambads\n",
    "\n",
    "Furthermore:\n",
    "* Study the bias-variance trade-off for varying parameters lambda (for Ridge) using bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge(x_value,y_value, lamb):\n",
    "    return np.linalg.pinv(x_value.T.dot(x_value) + lamb*np.identity(x_value.shape[1])).dot(x_value.T).dot(y_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap analysis (Ex. 2) using Ridge, with different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bias_variance_bootstrap_ridge_or_lasso(reg_func, x=None, y=None, z = None, noise = 0.1, nlambdas = 10, data_point_list = [20, 50, 100], save_fig = False, max_degree = 12, ):\n",
    "    #np.random.seed(88)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    mse_aggregate = np.zeros([len(data_point_list), nlambdas])\n",
    "    for data_point_index, num_points in enumerate(data_point_list):\n",
    "        #print(f\"Number of datapoints {num_points}\")\n",
    "        global_best_mse = float('inf')\n",
    "        best_lamb = 0\n",
    "        global_best_complexity = 0\n",
    "        \n",
    "        global_best_error_per_points = None\n",
    "        global_best_bias_per_points = None\n",
    "        global_best_variance_per_points = None\n",
    "        \n",
    "        global_best_test_split = 0\n",
    "        \n",
    "        best_extra_text = \"\"\n",
    "        \n",
    "        for lamb_ind, lamb in enumerate(lambdas):\n",
    "            extra =  'Lambda_'+str(lamb)\n",
    "            best_mse_this_lambda, best_complexity, best_error_per_points, best_bias_per_points, best_variance_per_points, best_train_split = run_bias_variance_bootstrap(reg_func, x=x, y=y, z=z, noise = noise, points = [num_points], max_degree = max_degree, lamb = lamb, plot_best = False, extra_text = extra)\n",
    "            \n",
    "            if best_mse_this_lambda < global_best_mse:\n",
    "                global_best_mse = best_mse_this_lambda\n",
    "                best_lamb = lamb\n",
    "                global_best_complexity = best_complexity\n",
    "                \n",
    "                global_best_error_per_points = best_error_per_points\n",
    "                global_best_bias_per_points = best_bias_per_points\n",
    "                global_best_variance_per_points = best_variance_per_points\n",
    "                \n",
    "                global_best_train_split = best_train_split\n",
    "                best_extra_text = extra\n",
    "            \n",
    "                \n",
    "        print(f\"Best lambda: {best_lamb}  for number of datapoints: {num_points}\")\n",
    "        \n",
    "        bias_annd_variance = np.add(global_best_bias_per_points.copy(), global_best_variance_per_points.copy())\n",
    "        polydegree =  np.arange(1, max_degree + 1)\n",
    "        print(f\"Best global: {global_best_mse} at complexity: {global_best_complexity} for number of datapoints: {num_points}, num_bootstraps = {num_points}, test_split = {global_best_train_split}\")\n",
    "        plt.figure(figsize = (15,5))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(polydegree, global_best_error_per_points.reshape(-1,1), label='Error')\n",
    "        plt.plot(polydegree, global_best_bias_per_points.reshape(-1,1), label='bias')\n",
    "        plt.plot(polydegree, global_best_variance_per_points.reshape(-1,1), label='Variance')\n",
    "        plt.plot(polydegree, bias_annd_variance.reshape(-1,1), label = 'bias + variance')\n",
    "        plt.title(f\"{reg_func.__name__}: Bias-Variance in linear scale.\\nNum_datapoints = {num_points}, \"+best_extra_text)\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return mse_aggregate, lambdas\n",
    "\n",
    "for num_points in [20, 50, 100]:\n",
    "    run_bias_variance_bootstrap_ridge_or_lasso(Ridge, data_point_list = [num_points], nlambdas = 40, save_fig = True)\n",
    "    run_bias_variance_bootstrap(least_square, points = [num_points], save_fig = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with a large regularization term, the number of data points will have less to say. The regularization term squishes the parameters of the model so much that the feeding more won't do much to make a better fit (unless you feed a lot).\n",
    "\n",
    "Remember that ridge shrinks larger weights -> they become more uniform. Too large regularization = no more weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation analysis (Ex. 3) using Ridge, with different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_analysis_ridge_lasso(reg_func ,x=None, y=None, z = None, noise = 0.1, nlambdas = 10, max_complexity = 10, data_points = [20, 50, 100], min_fold = 5, max_fold = 10, scale_with_std = False):\n",
    "    np.random.seed(88)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    mse_aggregate = np.zeros([3, max_fold - min_fold+1, max_complexity, nlambdas])\n",
    "    if z is not None:\n",
    "        data_points = [len(z)]\n",
    "    for data_point_index, data_points in enumerate(data_points):\n",
    "        print(f\"Number of datapoints {data_points}\")\n",
    "        global_best_mse = float('inf')\n",
    "        global_best_complexity = 0\n",
    "        global_best_mse_graph = None\n",
    "        best_num_bstraps = 0\n",
    "        best_lamb = 0\n",
    "        for lamb_ind, lamb in enumerate(lambdas):\n",
    "            all_mse_per_fold = full_cross_valid(reg_func, data_points, x=x, y=y, z=z, noise = noise, max_fold = max_fold, min_fold = min_fold, max_complexity = max_complexity, lamb=lamb, plot = False, show_plot = False, scale_with_std=scale_with_std)\n",
    "            best_fold, best_complexity = np.unravel_index(np.argmin(all_mse_per_fold, axis=None), all_mse_per_fold.shape)\n",
    "            best_mse = all_mse_per_fold[best_fold, best_complexity]\n",
    "            #print(lamb, best_mse, global_best_mse)\n",
    "            if best_mse < global_best_mse:\n",
    "                global_best_mse = best_mse\n",
    "                global_best_mse_graph = all_mse_per_fold[best_fold, :]\n",
    "                actual_best_fold = best_fold\n",
    "                global_best_complexity = best_complexity + 1\n",
    "                best_num_bstraps = data_points\n",
    "                best_lamb = lamb\n",
    "            mse_aggregate[data_point_index, :, :, lamb_ind] = all_mse_per_fold\n",
    "\n",
    "        actual_best_fold = actual_best_fold + min_fold \n",
    "        print(f\"Best params: Folds = {actual_best_fold} and complexity = {global_best_complexity} with lambda = {best_lamb}. Best MSE: {global_best_mse}\")\n",
    "        polydegree =  np.arange(1, max_complexity +1)\n",
    "        plt.figure(figsize = (16,9))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(polydegree, global_best_mse_graph)\n",
    "        plt.title(label=f\"Best MSE for CV using {reg_func.__name__} with lambda = {best_lamb}.\\nBest Params: Complexity = {global_best_complexity}, Best fold = {actual_best_fold}. {data_points} data points.\")\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        #plt.yscale('log')\n",
    "        plt.show()\n",
    "    return mse_aggregate, lambdas\n",
    "\n",
    "\n",
    "def plot_lambda_dependency(reg_func, plot_everything = False):\n",
    "    mse_aggregate, lambdas = cv_analysis_ridge_lasso(reg_func, nlambdas = 20, max_complexity = 10, min_fold = 5, max_fold = 10)\n",
    "    if plot_everything:\n",
    "        # for number of datapoints [20, 50, 100]\n",
    "        for data_points in mse_aggregate:\n",
    "            # for number of folds (5-10)\n",
    "            for fold, folds in enumerate(data_points):\n",
    "                # for complexity 0 -> max_complexity-1\n",
    "                for comp, complexity in enumerate(folds):\n",
    "                    plt.figure(figsize = (16,9))\n",
    "                    plt.tight_layout()\n",
    "                    plt.plot(np.log10(lambdas), complexity, label = 'MSE per lambda')\n",
    "                    plt.xlabel('log10(lambda)')\n",
    "                    plt.ylabel('mse')\n",
    "                    plt.title(f\"MSE per lambda for {len(complexity)} datapoints, for {(fold+5)} folds, for complexity {comp}\")\n",
    "                    plt.legend()\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lambda_dependency(Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the above analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_aggregate, lambdas = cv_analysis_ridge_lasso(Ridge, nlambdas = 20, max_complexity = 10, min_fold = 5, max_fold = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see how the regularization term affects the MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error_by_lambda(reg_func, complexity = 20):\n",
    "    lambdas = np.logspace(-5,2, 30)\n",
    "    \n",
    "    all_mse = np.zeros((len(lambdas), complexity))\n",
    "    all_r2 = np.zeros((len(lambdas), complexity))\n",
    "    for i, lamb in enumerate(lambdas):\n",
    "        mse, r2 = simple_mse_and_r2_by_complexity(reg_func, num_points = 1000, lamb = lamb, complexity = complexity, noise = 0.1, plot_mse = False, plot_r2 = False, return_losses = True)\n",
    "        all_mse[i,:] = np.array(mse)\n",
    "        all_r2[i,:] = np.array(r2)\n",
    "        \n",
    "    for comp in range(1, complexity):\n",
    "        plt.figure(figsize = (16,9))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(np.log10(lambdas), all_mse[:, comp], label = 'MSE per lambda')\n",
    "        plt.xlabel('log10(lambda)')\n",
    "        plt.ylabel('mse')\n",
    "        plt.title(f\"MSE per lambda for {reg_func.__name__} for complexity {comp}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.figure(figsize = (16,9))\n",
    "        plt.tight_layout()\n",
    "        plt.plot(np.log10(lambdas), all_r2[:, comp], label = 'MSE per lambda')\n",
    "        plt.xlabel('log10(lambda)')\n",
    "        plt.ylabel('r2')\n",
    "        plt.title(f\"R2 per lambda for {reg_func.__name__} for complexity {comp}\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_by_lambda(Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def Lasso(x,y, lamb):\n",
    "    \"\"\"\n",
    "    A hybrid implementation of SKlearn and our custom code.\n",
    "    We only use the coeficients from SKlearn, so that we can reuse our own code.\n",
    "    \"\"\"\n",
    "    lasso = linear_model.Lasso(fit_intercept = False, alpha = lamb, max_iter = 1000, tol = 0.05)\n",
    "    lasso.fit(x,y)\n",
    "    return lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Lasso vs. Least Square Bias variance trade off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bootstrap_comparison_ols_lasso():\n",
    "    for num_points in [20, 50, 100]:\n",
    "        run_bias_variance_bootstrap_ridge_or_lasso(Lasso, data_point_list = [num_points], nlambdas = 40, save_fig = True)\n",
    "        run_bias_variance_bootstrap(least_square, points = [num_points], save_fig = True)\n",
    "        \n",
    "run_bootstrap_comparison_ols_lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_aggregate, lambdas = cv_analysis_ridge_lasso(Lasso, nlambdas = 20, max_complexity = 10, min_fold = 5, max_fold = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again, let's see how lambda affect MSE with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_by_lambda(Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Terrain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the terrain\n",
    "terrain1 = imread(\"./data/SRTM_data_Norway_1.tif\")\n",
    "# Show the terrain\n",
    "plt.figure()\n",
    "plt.title(\"Terrain over Norway - Stavanger\")\n",
    "plt.imshow(terrain1, cmap=\"gray\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str = \"./data/SRTM_data_Norway_1.tif\", re_size = 10, display: bool = False):\n",
    "    \"\"\"\n",
    "    Load terrain dataset from image and create x and y coordinates.\n",
    "    \n",
    "    Returns:\n",
    "        x: x components of the input data\n",
    "        y: y components of the input data\n",
    "        z: targets \n",
    "    \"\"\"\n",
    "\n",
    "    data = imread(file_name)\n",
    "\n",
    "    if display:\n",
    "        plt.imshow(data, plt.cm.gray)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    np_terrain = np.array(data)\n",
    "    # Want the x and y values to be the axis.\n",
    "    x = np.linspace(0, 1, np_terrain.shape[0])\n",
    "    y = np.linspace(0, 1, np_terrain.shape[1])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Since x has double the length than y, we jump double the length to more effectively reduce the size of the dataset (for faster runs)\n",
    "    x = x[0::2*re_size, 0::re_size].reshape(-1,1)\n",
    "    y = y[0::2*re_size, 0::re_size].reshape(-1,1)\n",
    "    data = data.T[0::2*re_size, 0::re_size].reshape(-1,1) #Transpose to match the size of x and y.\n",
    "\n",
    "    return x, y, data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = load_data(re_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some simple visualization of the MSE and R2 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, xtx = simple_mse_and_r2_by_complexity(least_square, x = x, y=y, z=z, complexity = 30, noise = 0.1, plot_mse = True, plot_r2 = True, scale_with_std =  True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lamb in np.logspace(-4,2, 4):\n",
    "    print(lamb)\n",
    "    betas, xtx = simple_mse_and_r2_by_complexity(Ridge, x = x, y=y, z=z, lamb = lamb, complexity = 30, noise = 0.1, plot_mse = True, plot_r2 = True, scale_with_std =  True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lamb in np.logspace(-4,2, 4):\n",
    "    print(lamb)\n",
    "    betas, xtx = simple_mse_and_r2_by_complexity(Lasso, x = x, y=y, z=z, lamb = lamb, complexity = 30, noise = 0.1, plot_mse = True, plot_r2 = True, scale_with_std =  True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation for Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_aggregate, lambdas = cv_analysis_ridge_lasso(Ridge, x = x, y=y, z=z, nlambdas = 15, max_complexity = 10, min_fold = 5, max_fold = 15, scale_with_std = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_aggregate, lambdas = cv_analysis_ridge_lasso(Lasso, x = x, y=y, z=z, nlambdas = 15, max_complexity = 10, min_fold = 5, max_fold = 15, scale_with_std = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
