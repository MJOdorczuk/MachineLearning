{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate our own dataset for the Franke Function, with $x,y\\in[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our own code to perform a standard least square regression analysis usning polynomials in x and y up to fith order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "np.random.seed(np.random.randint(1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankeFunction(x, y, sigma = 0):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "\n",
    "    noise = np.random.normal(0, 1, x.shape[0])\n",
    "    noise = noise.reshape(x.shape[0],1)\n",
    "\n",
    "    return (term1 + term2 + term3 + term4).reshape(-1,1)  + sigma*noise\n",
    "\n",
    "def create_X(x, y, n ):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2)   # Number of elements in beta\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X\n",
    "\n",
    "def least_square(x_value,y_value, *args, **kwargs):\n",
    "    # Using pinv\n",
    "    return np.linalg.pinv(x_value.transpose().dot(x_value)).dot(x_value.transpose().dot(y_value))\n",
    "\n",
    "def plot_errors(x_range_train, x_range_test, y_values_train, y_values_test, title, xlabel_axis, ylabel_axis, graph_label_train, graph_label_test):\n",
    "    fig = plt.figure(figsize = (20,5))\n",
    "    plt.xlabel(xlabel_axis)\n",
    "    plt.ylabel(ylabel_axis)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_range_train, y_values_train, label=graph_label_train)\n",
    "    plt.plot(x_range_test, y_values_test, label=graph_label_test)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ols_run(\n",
    "    x = (np.random.uniform(0, 1, 1000)), \n",
    "    y =  (np.random.uniform(0, 1, 1000)), \n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_points != len(x):\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "        \n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "# Ridge: fit_intesect = False, da bryr vi oss ikke om intersect\n",
    "\n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        ols_beta = least_square(X_train, z_train)\n",
    "        all_ols_betas.append(ols_beta)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(ols_beta)\n",
    "        z_pred = X_test.dot(ols_beta)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "    \n",
    "    if plot_mse:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test'\n",
    "        )\n",
    "\n",
    "    if plot_r2:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n",
    "\n",
    "betas, xtx = simple_ols_run(num_points = 1000, complexity = 20, noise = 0.1, plot_mse = True, plot_r2 = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(beta, xtx_inv, confidence):\n",
    "    \"\"\"\n",
    "    Calculates the confidence interval of our parameters beta\n",
    "    \n",
    "    Args:\n",
    "        beta: our parameters\n",
    "        xtx_inv: (pseudo) inverted of our design matrix transposed multiplied with the design matrix.\n",
    "        confidence: confidence level\n",
    "    Returns:\n",
    "        confidence_interavl: the confidence interval for each of our parameters\n",
    "    \"\"\"\n",
    "    diag_sqrt = np.sqrt(np.diag(xtx_inv))\n",
    "    \n",
    "    confidence_interval = [beta-confidence*diag_sqrt, beta+confidence*diag_sqrt]\n",
    "        \n",
    "    return np.array(confidence_interval)\n",
    "\n",
    "betas, xtx = simple_ols_run(num_points = 100000, complexity = 5, noise = 10)\n",
    "confidence_interval(betas[-1], xtx[-1], 1.96), betas[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Figure 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, xtx = simple_ols_run(num_points = 100, complexity = 20, noise = 1, plot_mse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and variance explaination and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance trade-off analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance trade-off without bootstrap with constant data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get no variance, and that the bias = error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_analysis(reg_func, num_points, max_degree, lamb = 0):\n",
    "    np.random.seed(10)\n",
    "    # Make data\n",
    "    x = (np.random.uniform(0, 1, num_points))\n",
    "    y =  (np.random.uniform(0, 1, num_points))\n",
    "    z = FrankeFunction(x, y, 0.4) # Target\n",
    "    \n",
    "    # Aggregrate results\n",
    "    error = np.zeros(max_degree)\n",
    "    bias = np.zeros(max_degree)\n",
    "    variance = np.zeros(max_degree)\n",
    "    \n",
    "    for complexity in range(max_degree):\n",
    "        \n",
    "        # Make design matrix\n",
    "        X = create_X(x, y, n=complexity)\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        # Scaling the data\n",
    "        \"\"\"\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        X_train = scaler_in.transform(X_train)\n",
    "        X_test = scaler_in.transform(X_test)\n",
    "        z_train = scale_z.transform(z_train)\n",
    "        z_test = scale_z.transform(z_test)\n",
    "        \"\"\"\n",
    "        # Find optimal beta\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        \n",
    "        # Predict\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "        \n",
    "        # Loss:\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        \n",
    "        # Aggregrate stats:\n",
    "        error[complexity] = mse_test\n",
    "        bias[complexity] = np.mean((z_test - np.mean(z_pred, axis=1, keepdims=True))**2)\n",
    "        variance[complexity] = np.mean((z_pred - np.mean(z_pred, axis = 1, keepdims = True))**2)\n",
    "        \n",
    "    polydegree =  np.arange(max_degree)\n",
    "    plt.plot(polydegree, error, label='Error')\n",
    "    plt.plot(polydegree, bias, label='bias')\n",
    "    plt.plot(polydegree, variance, label='Variance')\n",
    "    plt.xlabel(\"Complexity\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    stuff = np.add(bias,variance)\n",
    "    print(variance)\n",
    "    print(error - stuff)\n",
    "    #print(variance)\n",
    "    #print(np.mean(z_pred, axis = 1, keepdims = True))\n",
    "    #print(z_pred.shape)\n",
    "    #print(np.abs(z_pred-np.mean(z_pred, axis = 0, keepdims = True)))\n",
    "    \"\"\"\n",
    "    \n",
    "bias_variance_analysis(least_square, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance trade-off with bootstrap and varying number of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse_with_broadcast(y_pred, y_test):\n",
    "    return np.mean((y_pred - y_test)**2)\n",
    "\n",
    "def bias_variance_analysis_bootstrap(reg_func, num_points, max_degree, num_bootstraps, lamb = 0, plot = True, show_plot = True):\n",
    "    # Make data\n",
    "    x = (np.random.uniform(0, 1, num_points))\n",
    "    y =  (np.random.uniform(0, 1, num_points))\n",
    "    z = FrankeFunction(x, y, 0.1) # Target\n",
    "    \n",
    "    # Aggregrate results\n",
    "    error = np.zeros(max_degree)\n",
    "    bias = np.zeros(max_degree)\n",
    "    variance = np.zeros(max_degree)\n",
    "    \n",
    "    for complexity in range(max_degree):\n",
    "        \n",
    "        # Make design matrix\n",
    "        X = create_X(x, y, n=complexity)\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        # Scaling the data\n",
    "        \n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        X_train = scaler_in.transform(X_train)\n",
    "        X_test = scaler_in.transform(X_test)\n",
    "        z_train = scale_z.transform(z_train)\n",
    "        z_test = scale_z.transform(z_test)\n",
    "        \n",
    "        # Bootstrap\n",
    "        z_pred_aggregate = np.empty((z_test.shape[0], num_bootstraps+1))\n",
    "        for boot_strap_number in range(1, num_bootstraps+1):\n",
    "            x_sample, y_sample = resample(X_train, z_train)\n",
    "            # Find optimal betas\n",
    "            beta_opt = reg_func(x_sample, y_sample, lamb)\n",
    "            # Predict\n",
    "            z_pred = X_test.dot(beta_opt).ravel()\n",
    "            # Aggregate the predictions\n",
    "            z_pred_aggregate[:, boot_strap_number] = z_pred\n",
    "        \n",
    "        # Loss: Mean of all the bootstrap MSE's\n",
    "        mse_test = np.mean(custom_mse_with_broadcast(z_pred_aggregate, z_test))\n",
    "        \n",
    "        # Aggregrate stats:\n",
    "        error[complexity] = mse_test\n",
    "        bias[complexity] = np.mean((z_test - np.mean(z_pred_aggregate, axis=1, keepdims=True))**2)\n",
    "        variance[complexity] = np.mean((z_pred_aggregate - np.mean(z_pred_aggregate, axis = 1, keepdims = True))**2)\n",
    "        \n",
    "    \n",
    "    if plot:  \n",
    "        polydegree =  np.arange(max_degree)\n",
    "        plt.plot(polydegree, error, label='Error')\n",
    "        plt.plot(polydegree, bias, label='bias')\n",
    "        plt.plot(polydegree, variance, label='Variance')\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.legend()\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "            \n",
    "    return error\n",
    "    \n",
    "for data_points in range(20, 100, 10):\n",
    "    print(data_points)\n",
    "    bias_variance_analysis_bootstrap(least_square, data_points, data_points, num_bootstraps = 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data after we split the data, as if we split it before, then the we scale based on all the data instead of just training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_custom(reg_func, num_points, num_splits, complexity, lamb = 0):\n",
    "    \n",
    "    # Create data\n",
    "    x = (np.random.uniform(0, 1, num_points))\n",
    "    y =  (np.random.uniform(0, 1, num_points))\n",
    "    z = FrankeFunction(x, y, 0.1) # Target\n",
    "    \n",
    "    X = create_X(x, y, n=complexity)\n",
    "\n",
    "    # Initialize our kfold\n",
    "    kfold = KFold(n_splits = num_splits)\n",
    "    \n",
    "    # Error\n",
    "    mse_test_kfold = np.zeros(num_splits)\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        # Training split + scaling\n",
    "        X_train = X[train_index] - np.mean(X[train_index])\n",
    "        z_train = z[train_index] - np.mean(z[train_index])\n",
    "\n",
    "        # Testing split + scaling\n",
    "        X_test = X[test_index] - np.mean(X[test_index])\n",
    "        z_test = z[test_index] - np.mean(z[test_index])\n",
    "        \n",
    "        # Optimal Betas\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        \n",
    "        # Prediction\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "        \n",
    "        mse_test_kfold[i] = mean_squared_error(z_test, z_pred)\n",
    "        \n",
    "        \n",
    "    return np.mean(mse_test_kfold)\n",
    "\n",
    "    \n",
    "def cross_validation(reg_func, num_points, num_folds, max_complexity, lamb=0, plot = False):\n",
    "    \n",
    "    mse_per_complexity = np.zeros(max_complexity)\n",
    "    \n",
    "    for complexity in range(max_complexity):\n",
    "        mse_per_complexity[complexity] = kfold_custom(reg_func, num_points, num_folds, complexity, lamb=lamb)\n",
    "    \n",
    "    if plot:\n",
    "        polydegree =  np.arange(max_complexity)\n",
    "        plt.plot(polydegree, mse_per_complexity, label='MSE cross validation: Num Folds = ' + str(num_folds))\n",
    "        plt.xlabel(\"Complexity\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.legend()\n",
    "    \n",
    "    return mse_per_complexity\n",
    "        \n",
    "def full_cross_valid(reg_func, num_points, min_fold, max_fold, max_complecity, lamb=0, plot = False, show_plot = False):\n",
    "    \n",
    "    for fold in range(min_fold, max_fold+1):\n",
    "        cross_validation(reg_func, num_points, fold, max_complecity, lamb=lamb, plot=plot)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "\n",
    "full_cross_valid(least_square, 100, 5,10 , 10, plot = True, show_plot = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_sklearn(num_points, num_folds, max_complexity, plot=False):\n",
    "    estimated_mse_sklearn = np.zeros(max_complexity)\n",
    "    \n",
    "    x = (np.random.uniform(0, 1, num_points))\n",
    "    y =  (np.random.uniform(0, 1, num_points))\n",
    "    z = FrankeFunction(x, y, 0.1) # Target\n",
    "    \n",
    "    for complexity in range(max_complexity):\n",
    "        \n",
    "        # Don't scale, just fit the raw data.\n",
    "        linreg = LinearRegression(fit_intercept = False)\n",
    "        \n",
    "        X = create_X(x, y, n=complexity)\n",
    "        \n",
    "        kfold = KFold(n_splits = num_folds)\n",
    "\n",
    "        estimated_mse_folds = cross_val_score(linreg, X, z, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "        estimated_mse_sklearn[complexity] = np.mean(-estimated_mse_folds)\n",
    "        \n",
    "    polydegree =  np.arange(max_complexity)\n",
    "    plt.plot(polydegree, estimated_mse_sklearn, label='MSE cross validation Sklearn' + str(num_folds))\n",
    "    plt.xlabel(\"Complexity\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "cross_val_sklearn(100, 10, 10, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cross_valid_custom_vs_sklearn():\n",
    "    for fold in range(5,11):\n",
    "        full_cross_valid(least_square, 100, fold,fold,10, plot=True)\n",
    "        cross_val_sklearn(100, fold, 10)\n",
    "        plt.show()\n",
    "\n",
    "compare_cross_valid_custom_vs_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Cross validation vs. bootstrap for MSE for Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cv_with_booststrap(max_data_points, max_complexity):\n",
    "    for fold in range(5,11):\n",
    "        for num_data_points in range(20,max_data_points, 20):\n",
    "            bootstrap_mse = bias_variance_analysis_bootstrap(least_square, num_data_points, max_complexity, num_data_points, plot = False) \n",
    "            cv_mse = cross_validation(least_square, num_data_points, fold, max_complexity)\n",
    "\n",
    "            polydegree =  np.arange(max_complexity)\n",
    "            plt.plot(polydegree, bootstrap_mse, label='MSE Boot_strap')\n",
    "            plt.plot(polydegree, cv_mse, label='MSE CV num_folds: '+str(fold))\n",
    "            plt.xlabel(\"Complexity\")\n",
    "            plt.ylabel(\"MSE: num_data_points: \"+str(num_data_points))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "compare_cv_with_booststrap(100, 10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commment comment comment...\n",
    "\n",
    "Num datapoints: CV better for many, bootstrap better for few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will:\n",
    "* Write our own Ridge regression using singular value decomposition using the function pinv(), which uses the SVD.\n",
    "* Perform the same bootstrap analysis as we did in exercise 2, using the same polynomials with different lambdas.\n",
    "* Perform the same cross validation as in exercise 3, but with varying lambdas.\n",
    "* Compare the results from Ridge (Ex. 4) with results from Least Square (Ex. 1-3).\n",
    "* In general: Study the dependancies of lambads\n",
    "\n",
    "Furthermore:\n",
    "* Study the bias-variance trade-off for varying parameters lambda (for Ridge) using bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge(x_value,y_value, lamb):\n",
    "    return np.linalg.pinv(x_value.T.dot(x_value) + lamb*np.identity(x_value.shape[1])).dot(x_value.T).dot(y_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap analysis (Ex. 2) using Ridge, with different lambads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_analysis_ridge_lambas(nlambdas = 10):\n",
    "    np.random.seed(10)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    for data_points in [20, 50, 100]:\n",
    "        for lamb in lambdas:\n",
    "            print(f\"Lambda: {lamb}. Datapoints: {data_points}\")\n",
    "            bias_variance_analysis_bootstrap(Ridge, data_points, 10, data_points, lamb = lamb) \n",
    "            \n",
    "bootstrap_analysis_ridge_lambas(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with a large regularization term, no matter the number of data points will have less to say. The regularization term squishes the parameters of the model so much that the feeding more won't do much to make a better fit (unless you feed a lot).\n",
    "\n",
    "Remember that ridge shrinks larger weights -> they become more uniform. Too large regularization = no more weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation analysis (Ex. 3) using Ridge, with different lambads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_analysis_ridge_lambas(reg_func, num_points, min_fold, max_fold, max_complecity, nlambdas = 10, plot = False, show_plot = False):\n",
    "    np.random.seed(10)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    for lamb in lambdas:\n",
    "        plt.figure(figsize=(20,5))\n",
    "        for fold in range(min_fold, max_fold+1):\n",
    "            cross_validation(reg_func, num_points, fold, max_complecity, lamb=lamb, plot=plot)\n",
    "\n",
    "        if show_plot:\n",
    "            plt.title(f\"Lambda: {lamb}\")\n",
    "            plt.show()\n",
    "bootstrap_analysis_ridge_lambas(Ridge, 40, 5, 10, 10,  nlambdas = 10, plot= True, show_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ridge_run(\n",
    "    x = (np.random.uniform(0, 1, 1000)), \n",
    "    y =  (np.random.uniform(0, 1, 1000)), \n",
    "    lamb = 0,\n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_points != len(x):\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "        \n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "# Ridge: fit_intesect = False, da bryr vi oss ikke om intersect\n",
    "\n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        ols_beta = Ridge(X_train, z_train, lamb)\n",
    "        all_ols_betas.append(ols_beta)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(ols_beta)\n",
    "        z_pred = X_test.dot(ols_beta)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "    \n",
    "    if plot_mse:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity. Lambda = '+str(lamb), \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test'\n",
    "        )\n",
    "\n",
    "    if plot_r2:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity. Lambda = '+str(lamb), \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, xtx = simple_ols_run(num_points = 1000, complexity = 20, noise = 0.1, plot_mse = True, plot_r2 = True)\n",
    "lambdas = np.logspace(-4,2, 5)\n",
    "for lamb in lambdas:\n",
    "    betas, xtx = simple_ridge_run(num_points = 1000, lamb = lamb, complexity = 20, noise = 0.1, plot_mse = True, plot_r2 = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
