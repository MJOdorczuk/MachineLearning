{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_analysis_ridge_lambas(nlambdas = 10):\n",
    "    np.random.seed(10)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    mse_aggregate = np.zeros([3, nlambdas])\n",
    "    best_complexities = np.zeros([3,nlambdas])\n",
    "    for data_point_index, data_points in enumerate([20, 50, 100]):\n",
    "        print(f\"Number of datapoints {data_points}\")\n",
    "        global_best_mse = float('inf')\n",
    "        global_best_complexity = 0\n",
    "        best_num_bstraps = 0\n",
    "        best_train_split = 0\n",
    "        best_lamb = 0\n",
    "        for lamb_ind, lamb in enumerate(lambdas):\n",
    "            best_mse, best_complexity = bias_variance_analysis_bootstrap(Ridge, data_points, max_degree=10, num_bootstraps = data_points, lamb = lamb, plot_log = False, plot = False)\n",
    "            #print(f\"Lambda: {lamb}. Datapoints: {data_points}. MSE: {best_mse}\")\n",
    "            if best_mse < global_best_mse:\n",
    "                global_best_mse = best_mse\n",
    "                global_best_complexity = best_complexity\n",
    "                best_num_bstraps = data_points\n",
    "                best_lamb = lamb\n",
    "            \n",
    "            mse_aggregate[data_point_index, lamb_ind] = best_mse\n",
    "            best_complexities[data_point_index, lamb_ind] = best_complexity\n",
    "                \n",
    "        print(f\"Best lambda: {best_lamb} Best global: {global_best_mse} at complexity: {global_best_complexity} for number of datapoints: {data_points}, num_bootstraps = {data_points}\")\n",
    "        print(bias_variance_analysis_bootstrap(Ridge, data_points, max_degree=10, num_bootstraps = best_num_bstraps, plot=True, plot_log=False,  lamb = best_lamb))\n",
    "        \n",
    "    return mse_aggregate, lambdas, best_complexities\n",
    "\n",
    "\n",
    "def plot_lambda_dependency():\n",
    "    mse_aggregate, lambdas, complexity = bootstrap_analysis_ridge_lambas(100)\n",
    "    for i, data_points in enumerate(mse_aggregate):\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('log10(lambda)')\n",
    "        ax1.set_ylabel('mse')\n",
    "        ax1.plot(np.log10(lambdas), data_points, label = 'MSE per lambda', color = color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Complexity', color=color)\n",
    "        ax2.plot(np.log10(lambdas), complexity[i], color=color, label=\"Best Complexity\")\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped    \n",
    "        plt.title(f\"MSE per lambda for {len(data_points)} datapoints\")\n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_lambda_dependency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_analysis_ridge_lambas(num_points, min_fold, max_fold, max_complecity, nlambdas = 10, plot = False, show_plot = False):\n",
    "    np.random.seed(88)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    for lamb in lambdas:\n",
    "        plt.figure(figsize=(20,5))\n",
    "        for fold in range(min_fold, max_fold+1):\n",
    "            cross_validation(Ridge, num_points, fold, max_complecity, lamb=lamb, plot=plot)\n",
    "\n",
    "        if show_plot:\n",
    "            plt.title(f\"Lambda: {lamb}\")\n",
    "            plt.show()\n",
    "cv_analysis_ridge_lambas(40, 5, 10, 10,  nlambdas = 100, plot= True, show_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: The complexity of the different lambdas may vary. This is just a plot of the absolute best complexity of each of the lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ridge_run(\n",
    "    x = (np.random.uniform(0, 1, 1000)), \n",
    "    y =  (np.random.uniform(0, 1, 1000)), \n",
    "    lamb = 0,\n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_points != len(x):\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "        \n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "# Ridge: fit_intesect = False, da bryr vi oss ikke om intersect\n",
    "\n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        ols_beta = Ridge(X_train, z_train, lamb)\n",
    "        all_ols_betas.append(ols_beta)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(ols_beta)\n",
    "        z_pred = X_test.dot(ols_beta)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "    \n",
    "    if plot_mse:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity. Lambda = '+str(lamb), \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test'\n",
    "        )\n",
    "\n",
    "    if plot_r2:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity. Lambda = '+str(lamb), \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mse_and_r2_by_complexity(\n",
    "    reg_func = least_square,\n",
    "    x = (np.random.uniform(0, 1, 1000)), \n",
    "    y =  (np.random.uniform(0, 1, 1000)), \n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False,\n",
    "    y_scale = 'linear',\n",
    "    lamb = 0):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_points != len(x):\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "        \n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "    \n",
    "    lass_mse_train = []\n",
    "    lass_mse_test = []\n",
    "    lass_r2_train = []\n",
    "    lass_r2_test = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        all_ols_betas.append(beta_opt)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(beta_opt)\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "        \n",
    "        lasso = linear_model.Lasso(fit_intercept = False, alpha = lamb, max_iter = 10000, tol = 0.005)\n",
    "        lasso.fit(X_train, z_train)\n",
    "        z_tilde = lasso.predict(X_train)\n",
    "        z_pred = lasso.predict(X_test)\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        lass_mse_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        lass_mse_test.append(mse_test)\n",
    "\n",
    "        lass_r2_train.append(r2_score(z_tilde, z_train))\n",
    "        lass_r2_test.append(r2_score(z_pred, z_test))\n",
    "        \n",
    "    \n",
    "    if plot_mse:\n",
    "        plt.figure(figsize = (20,5))\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test',\n",
    "            y_scale = y_scale\n",
    "        )\n",
    "        \n",
    "        \n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = lass_mse_train, \n",
    "            y_values_test = lass_mse_test,\n",
    "            title = 'MSE by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train_lasso',\n",
    "            graph_label_test = 'mse_test_lasso',\n",
    "            y_scale = y_scale\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    if plot_r2:\n",
    "        plt.figure(figsize = (20,5))\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test'\n",
    "        )\n",
    "        \n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = lass_r2_train, \n",
    "            y_values_test = lass_r2_test,\n",
    "            title = 'R2 by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train_lasso',\n",
    "            graph_label_test = 'r2_test_lasso'\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n",
    "\n",
    "betas, xtx = simple_mse_and_r2_by_complexity(num_points = 1000, complexity = 20, noise = 0.1, plot_mse = True, plot_r2 = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance trade-off without bootstrap with constant data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get no variance, and that the bias = error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import norm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from imageio import imread\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "np.random.seed(np.random.randint(1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32851, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(file_name: str = \"SRTM_data_Norway_1.tif\", re_size = 10, display: bool = False):\n",
    "    \"\"\"Load terrain dataset from image and create x and y coordinates.\n",
    "    Returns\n",
    "    -------\n",
    "    x, y, z\n",
    "    \"\"\"\n",
    "    data = imread(\"./data/SRTM_data_Norway_1.tif\")\n",
    "\n",
    "    if display:\n",
    "        plt.imshow(data, plt.cm.gray)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    np_terrain1 = np.array(data)\n",
    "    x = np.linspace(0, 1, np_terrain1.shape[0])\n",
    "    y = np.linspace(0, 1, np_terrain1.shape[1])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    x = x[0::2*re_size, 0::re_size].reshape(-1,1)\n",
    "    y = y[0::2*re_size, 0::re_size].reshape(-1,1)\n",
    "    data = data.T[0::2*re_size, 0::re_size].reshape(-1,1) #Transpose, because because \n",
    "\n",
    "    return x, y, data\n",
    "x,y,data = load_data()\n",
    "x.shape, y.shape, data.shape\n",
    "create_X(x,y, 1).shape #(65341, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankeFunction(x, y, sigma = 0):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "\n",
    "    noise = np.random.normal(0, 1, x.shape[0])\n",
    "    noise = noise.reshape(x.shape[0],1)\n",
    "\n",
    "    return (term1 + term2 + term3 + term4).reshape(-1,1)  + sigma*noise\n",
    "\n",
    "def create_X(x, y, n ):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2)   # Number of elements in beta\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X\n",
    "\n",
    "def least_square(x_value,y_value, *args, **kwargs):\n",
    "    # Using pinv\n",
    "    return np.linalg.pinv(x_value.transpose().dot(x_value)).dot(x_value.transpose().dot(y_value))\n",
    "\n",
    "def plot_errors(x_range_train, x_range_test, y_values_train, y_values_test, title, xlabel_axis, ylabel_axis, graph_label_train, graph_label_test, y_scale = 'linear'):\n",
    "    fig = plt.figure(figsize = (20,5))\n",
    "    plt.xlabel(xlabel_axis)\n",
    "    plt.ylabel(ylabel_axis)\n",
    "    plt.title(title)\n",
    "    plt.yscale(y_scale)\n",
    "    plt.plot(x_range_train, y_values_train, label=graph_label_train)\n",
    "    plt.plot(x_range_test, y_values_test, label=graph_label_test)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_analysis(reg_func, num_points, max_degree, lamb = 0):\n",
    "    #np.random.seed(88)\n",
    "    # Make data\n",
    "    x = (np.random.uniform(0, 1, num_points))\n",
    "    y =  (np.random.uniform(0, 1, num_points))\n",
    "    z = FrankeFunction(x, y, 0.4) # Target\n",
    "    def load_data(file_name: str = \"SRTM_data_Norway_1.tif\", re_size = 0, display: bool = False) -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"Load terrain dataset from image and create x and y coordinates.\n",
    "    Returns\n",
    "    -------\n",
    "    x, y, z\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(\n",
    "        os.path.dirname(__file__),\n",
    "        file_name,\n",
    "    )\n",
    "    data = imageio.imread(image_path)\n",
    "\n",
    "    if display:\n",
    "        plt.imshow(data, plt.cm.gray)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    np_terrain1 = np.array(terrain1)\n",
    "    x = np.linspace(0, 1, np_terrain1.shape[0])\n",
    "    y = np.linspace(0, 1, np_terrain1.shape[1])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    x = x[0::2*10, 0::10].reshape(-1,1)\n",
    "    y = y[0::2*10, 0::10].reshape(-1,1)\n",
    "    data = data.T[0::2*10, 0::10].reshape(-1,1) #Transpose, because because \n",
    "\n",
    "    return x, y, data\n",
    "\n",
    "    # Aggregrate results\n",
    "    error = np.zeros(max_degree)\n",
    "    bias = np.zeros(max_degree)\n",
    "    variance = np.zeros(max_degree)\n",
    "    \n",
    "    for complexity in range(max_degree):\n",
    "        \n",
    "        # Make design matrix\n",
    "        X = create_X(x, y, n=complexity)\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        # Scaling the data\n",
    "        \"\"\"\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        X_train = scaler_in.transform(X_train)\n",
    "        X_test = scaler_in.transform(X_test)\n",
    "        z_train = scale_z.transform(z_train)\n",
    "        z_test = scale_z.transform(z_test)\n",
    "        \"\"\"\n",
    "        # Find optimal beta\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        \n",
    "        # Predict\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "        \n",
    "        # Loss:\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        \n",
    "        # Aggregrate stats:\n",
    "        error[complexity] = mse_test\n",
    "        bias[complexity] = np.mean((z_test - np.mean(z_pred, axis=1, keepdims=True))**2)\n",
    "        variance[complexity] = np.mean((z_pred - np.mean(z_pred, axis = 1, keepdims = True))**2)\n",
    "        \n",
    "    polydegree =  np.arange(max_degree)\n",
    "    plt.plot(polydegree, error, label='Error')\n",
    "    plt.plot(polydegree, bias, label='bias')\n",
    "    plt.plot(polydegree, variance, label='Variance')\n",
    "    plt.xlabel(\"Complexity\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    stuff = np.add(bias,variance)\n",
    "    print(variance)\n",
    "    print(error - stuff)\n",
    "    #print(variance)\n",
    "    #print(np.mean(z_pred, axis = 1, keepdims = True))\n",
    "    #print(z_pred.shape)\n",
    "    #print(np.abs(z_pred-np.mean(z_pred, axis = 0, keepdims = True)))\n",
    "    \"\"\"\n",
    "    \n",
    "bias_variance_analysis(least_square, 100, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
