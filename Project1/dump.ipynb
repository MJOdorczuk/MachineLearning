{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_analysis_ridge_lambas(nlambdas = 10):\n",
    "    np.random.seed(10)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    mse_aggregate = np.zeros([3, nlambdas])\n",
    "    best_complexities = np.zeros([3,nlambdas])\n",
    "    for data_point_index, data_points in enumerate([20, 50, 100]):\n",
    "        print(f\"Number of datapoints {data_points}\")\n",
    "        global_best_mse = float('inf')\n",
    "        global_best_complexity = 0\n",
    "        best_num_bstraps = 0\n",
    "        best_train_split = 0\n",
    "        best_lamb = 0\n",
    "        for lamb_ind, lamb in enumerate(lambdas):\n",
    "            best_mse, best_complexity = bias_variance_analysis_bootstrap(Ridge, data_points, max_degree=10, num_bootstraps = data_points, lamb = lamb, plot_log = False, plot = False)\n",
    "            #print(f\"Lambda: {lamb}. Datapoints: {data_points}. MSE: {best_mse}\")\n",
    "            if best_mse < global_best_mse:\n",
    "                global_best_mse = best_mse\n",
    "                global_best_complexity = best_complexity\n",
    "                best_num_bstraps = data_points\n",
    "                best_lamb = lamb\n",
    "            \n",
    "            mse_aggregate[data_point_index, lamb_ind] = best_mse\n",
    "            best_complexities[data_point_index, lamb_ind] = best_complexity\n",
    "                \n",
    "        print(f\"Best lambda: {best_lamb} Best global: {global_best_mse} at complexity: {global_best_complexity} for number of datapoints: {data_points}, num_bootstraps = {data_points}\")\n",
    "        print(bias_variance_analysis_bootstrap(Ridge, data_points, max_degree=10, num_bootstraps = best_num_bstraps, plot=True, plot_log=False,  lamb = best_lamb))\n",
    "        \n",
    "    return mse_aggregate, lambdas, best_complexities\n",
    "\n",
    "\n",
    "def plot_lambda_dependency():\n",
    "    mse_aggregate, lambdas, complexity = bootstrap_analysis_ridge_lambas(100)\n",
    "    for i, data_points in enumerate(mse_aggregate):\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('log10(lambda)')\n",
    "        ax1.set_ylabel('mse')\n",
    "        ax1.plot(np.log10(lambdas), data_points, label = 'MSE per lambda', color = color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Complexity', color=color)\n",
    "        ax2.plot(np.log10(lambdas), complexity[i], color=color, label=\"Best Complexity\")\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped    \n",
    "        plt.title(f\"MSE per lambda for {len(data_points)} datapoints\")\n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_lambda_dependency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_analysis_ridge_lambas(num_points, min_fold, max_fold, max_complecity, nlambdas = 10, plot = False, show_plot = False):\n",
    "    np.random.seed(88)\n",
    "    lambdas = np.logspace(-4,4, nlambdas)\n",
    "    for lamb in lambdas:\n",
    "        plt.figure(figsize=(20,5))\n",
    "        for fold in range(min_fold, max_fold+1):\n",
    "            cross_validation(Ridge, num_points, fold, max_complecity, lamb=lamb, plot=plot)\n",
    "\n",
    "        if show_plot:\n",
    "            plt.title(f\"Lambda: {lamb}\")\n",
    "            plt.show()\n",
    "cv_analysis_ridge_lambas(40, 5, 10, 10,  nlambdas = 100, plot= True, show_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: The complexity of the different lambdas may vary. This is just a plot of the absolute best complexity of each of the lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ridge_run(\n",
    "    x = (np.random.uniform(0, 1, 1000)), \n",
    "    y =  (np.random.uniform(0, 1, 1000)), \n",
    "    lamb = 0,\n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_points != len(x):\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "        \n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "# Ridge: fit_intesect = False, da bryr vi oss ikke om intersect\n",
    "\n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        ols_beta = Ridge(X_train, z_train, lamb)\n",
    "        all_ols_betas.append(ols_beta)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(ols_beta)\n",
    "        z_pred = X_test.dot(ols_beta)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "    \n",
    "    if plot_mse:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity. Lambda = '+str(lamb), \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test'\n",
    "        )\n",
    "\n",
    "    if plot_r2:\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity. Lambda = '+str(lamb), \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mse_and_r2_by_complexity(\n",
    "    reg_func = least_square,\n",
    "    x = (np.random.uniform(0, 1, 1000)), \n",
    "    y =  (np.random.uniform(0, 1, 1000)), \n",
    "    num_points = 1000, \n",
    "    complexity = 5, \n",
    "    noise = 0, \n",
    "    scale = True, \n",
    "    plot_mse = False, \n",
    "    plot_r2 = False,\n",
    "    y_scale = 'linear',\n",
    "    lamb = 0):\n",
    "    \"\"\"\n",
    "    Computes the simples ordinary least square based on the Franke Function\n",
    "    \n",
    "    Args:\n",
    "        stuff\n",
    "        \n",
    "    Returns:\n",
    "        ols_beta: The OLS\n",
    "    \"\"\"\n",
    "    \n",
    "    if num_points != len(x):\n",
    "        x = (np.random.uniform(0, 1, num_points))\n",
    "        y =  (np.random.uniform(0, 1, num_points))\n",
    "        \n",
    "        \n",
    "    MSE_train = []\n",
    "    MSE_pred = []\n",
    "    r2_train = []\n",
    "    r2_pred = []\n",
    "    \n",
    "    all_ols_betas = []\n",
    "    all_xtx_inv = []\n",
    "    \n",
    "    lass_mse_train = []\n",
    "    lass_mse_test = []\n",
    "    lass_r2_train = []\n",
    "    lass_r2_test = []\n",
    "\n",
    "    for complexity in range(1,complexity+1):\n",
    "\n",
    "        #Trying not to sort the x and y's\n",
    "        z = FrankeFunction(x, y, noise) # Target\n",
    "        X = create_X(x, y, n=complexity)  # Data\n",
    "\n",
    "        # True to z instead of y, and same with predictions: z_pred instead of y_pred\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        #scaler = MinMaxScaler(feature_range= [-1,1])\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        if scale:\n",
    "            X_train = scaler_in.transform(X_train)\n",
    "            X_test = scaler_in.transform(X_test)\n",
    "            #X_train -= np.mean(X_train)\n",
    "            #X_test -= np.mean(X_test)\n",
    "            z_train = scale_z.transform(z_train)\n",
    "            z_test = scale_z.transform(z_test)\n",
    "\n",
    "\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        all_ols_betas.append(beta_opt)\n",
    "        \n",
    "        xtx = np.linalg.pinv(X_train.transpose().dot(X_train))\n",
    "        all_xtx_inv.append(xtx)\n",
    "\n",
    "        z_tilde = X_train.dot(beta_opt)\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "\n",
    "\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        MSE_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        MSE_pred.append(mse_test)\n",
    "\n",
    "        r2_train.append(r2_score(z_tilde, z_train))\n",
    "        r2_pred.append(r2_score(z_pred, z_test))\n",
    "        \n",
    "        lasso = linear_model.Lasso(fit_intercept = False, alpha = lamb, max_iter = 10000, tol = 0.005)\n",
    "        lasso.fit(X_train, z_train)\n",
    "        z_tilde = lasso.predict(X_train)\n",
    "        z_pred = lasso.predict(X_test)\n",
    "        mse_train = mean_squared_error(z_tilde, z_train)\n",
    "        lass_mse_train.append(mse_train)\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        lass_mse_test.append(mse_test)\n",
    "\n",
    "        lass_r2_train.append(r2_score(z_tilde, z_train))\n",
    "        lass_r2_test.append(r2_score(z_pred, z_test))\n",
    "        \n",
    "    \n",
    "    if plot_mse:\n",
    "        plt.figure(figsize = (20,5))\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = MSE_train, \n",
    "            y_values_test = MSE_pred,\n",
    "            title = 'MSE by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train',\n",
    "            graph_label_test = 'mse_test',\n",
    "            y_scale = y_scale\n",
    "        )\n",
    "        \n",
    "        \n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = lass_mse_train, \n",
    "            y_values_test = lass_mse_test,\n",
    "            title = 'MSE by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'MSE',\n",
    "            graph_label_train = 'mse_train_lasso',\n",
    "            graph_label_test = 'mse_test_lasso',\n",
    "            y_scale = y_scale\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    if plot_r2:\n",
    "        plt.figure(figsize = (20,5))\n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = r2_train, \n",
    "            y_values_test = r2_pred,\n",
    "            title = 'R2 by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train',\n",
    "            graph_label_test = 'r2_test'\n",
    "        )\n",
    "        \n",
    "        plot_errors(\n",
    "            x_range_train = np.arange(1, complexity+1), \n",
    "            x_range_test = np.arange(1, complexity+1), \n",
    "            y_values_train = lass_r2_train, \n",
    "            y_values_test = lass_r2_test,\n",
    "            title = 'R2 by complexity', \n",
    "            xlabel_axis = 'Complexity',\n",
    "            ylabel_axis = 'R2 score',\n",
    "            graph_label_train = 'r2_train_lasso',\n",
    "            graph_label_test = 'r2_test_lasso'\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    return all_ols_betas, all_xtx_inv\n",
    "\n",
    "betas, xtx = simple_mse_and_r2_by_complexity(num_points = 1000, complexity = 20, noise = 0.1, plot_mse = True, plot_r2 = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance trade-off without bootstrap with constant data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get no variance, and that the bias = error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_analysis(reg_func, num_points, max_degree, lamb = 0):\n",
    "    #np.random.seed(88)\n",
    "    # Make data\n",
    "    x = (np.random.uniform(0, 1, num_points))\n",
    "    y =  (np.random.uniform(0, 1, num_points))\n",
    "    z = FrankeFunction(x, y, 0.4) # Target\n",
    "    \n",
    "    # Aggregrate results\n",
    "    error = np.zeros(max_degree)\n",
    "    bias = np.zeros(max_degree)\n",
    "    variance = np.zeros(max_degree)\n",
    "    \n",
    "    for complexity in range(max_degree):\n",
    "        \n",
    "        # Make design matrix\n",
    "        X = create_X(x, y, n=complexity)\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "        # Scaling the data\n",
    "        \"\"\"\n",
    "        scaler_in = StandardScaler(with_std=False)\n",
    "        scaler_in.fit(X_train)\n",
    "        scale_z = StandardScaler(with_std=False)\n",
    "        scale_z.fit(z_train)\n",
    "        \n",
    "        X_train = scaler_in.transform(X_train)\n",
    "        X_test = scaler_in.transform(X_test)\n",
    "        z_train = scale_z.transform(z_train)\n",
    "        z_test = scale_z.transform(z_test)\n",
    "        \"\"\"\n",
    "        # Find optimal beta\n",
    "        beta_opt = reg_func(X_train, z_train, lamb)\n",
    "        \n",
    "        # Predict\n",
    "        z_pred = X_test.dot(beta_opt)\n",
    "        \n",
    "        # Loss:\n",
    "        mse_test = mean_squared_error(z_pred, z_test)\n",
    "        \n",
    "        # Aggregrate stats:\n",
    "        error[complexity] = mse_test\n",
    "        bias[complexity] = np.mean((z_test - np.mean(z_pred, axis=1, keepdims=True))**2)\n",
    "        variance[complexity] = np.mean((z_pred - np.mean(z_pred, axis = 1, keepdims = True))**2)\n",
    "        \n",
    "    polydegree =  np.arange(max_degree)\n",
    "    plt.plot(polydegree, error, label='Error')\n",
    "    plt.plot(polydegree, bias, label='bias')\n",
    "    plt.plot(polydegree, variance, label='Variance')\n",
    "    plt.xlabel(\"Complexity\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    stuff = np.add(bias,variance)\n",
    "    print(variance)\n",
    "    print(error - stuff)\n",
    "    #print(variance)\n",
    "    #print(np.mean(z_pred, axis = 1, keepdims = True))\n",
    "    #print(z_pred.shape)\n",
    "    #print(np.abs(z_pred-np.mean(z_pred, axis = 0, keepdims = True)))\n",
    "    \"\"\"\n",
    "    \n",
    "bias_variance_analysis(least_square, 100, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
